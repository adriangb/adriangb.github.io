{"config":{"lang":["en"],"separator":"[\\s\\-]+","pipeline":["stopWordFilter"],"fields":{"title":{"boost":1000.0},"text":{"boost":1.0},"tags":{"boost":1000000.0}}},"docs":[{"location":"","title":"Blog","text":""},{"location":"2023/04/01/data-conversions-in-pandas-arrow-to-the-rescue/","title":"Data conversions in Pandas: Arrow to the rescue","text":"<p>An issue I've seen frequently in data engineering is is loss of type information when moving data between systems. This is especially bad when Pandas is in the mix, and it's one of the most widely used data engineering tools.</p>"},{"location":"2023/04/01/data-conversions-in-pandas-arrow-to-the-rescue/#loss-of-type-information-within-pandas","title":"Loss of type information within Pandas","text":"<p>Pandas' type system is generally quite broken. For a long time you could not have an int column with null values:</p> <pre><code>&gt;&gt;&gt; import pandas as pd\n&gt;&gt;&gt; # source data uses -1 to indicate missing, a pretty common scenario\n&gt;&gt;&gt; df = pd.DataFrame({\"x\": [-1, 2, 3]})\n&gt;&gt;&gt; # lets convert those to nones / nulls\n&gt;&gt;&gt; df['x'] = df['x'].map(lambda x: x if x != -1 else None)\n&gt;&gt;&gt; df\n&gt;&gt;&gt; df\nx\n0  NaN\n1  2.0\n2  3.0\n</code></pre> <p>What happened here? Our integer column is now floats! There are dozens of similar things that you'll run into when using Pandas. The good news is that this is pretty much a solved problem since Pandas now supports Arrow data types! You could also use Polars, but being able to switch Pandas' backend with just 1-2 LOC is very useful for existing codebases. And I expect this support to get better over time. If you haven't tried it yet, I highly recommend exploring this for at least pieces of your workflows.</p>"},{"location":"2023/04/01/data-conversions-in-pandas-arrow-to-the-rescue/#loss-of-type-information-at-the-edges","title":"Loss of type information at the edges","text":"<p>A lot of data workflows start with loading data and end with exporting that data. Often this will be loading or exporting to CSV and Parquet.</p> <p>Using Arrow as the in-memory format also alleviates a lot of issues with loading and exporting Parquet: while Parquet != Arrow, Arrow is the de-factor in memory storage format for Parquet data, so Parquet interop is excellent.</p> <p>Unfortunately that is not the case for CSVs: the format is fundamentally under-specified, inefficient and type unsafe. My suggestion: avoid it if you can, use Parquet or something else instead. The next post will discuss replacing CSVs with Parquet for the purposes of loading data into Postgres.</p> <p>Of course if you already have a good domain model described via Protocol Buffers or something like that, you could use that as well.</p>"},{"location":"2023/04/01/data-conversions-in-pandas-arrow-to-the-rescue/#empty-files","title":"Empty files","text":"<p>Empty files are a particularly good example of CSV breaking down as a format. Since CSVs are not self describing (they're literally just a big string) when you load a CSV (into Pandas or anything else really) the types of the columns have to be inferred. But it's not just the types: it's line endings, delimiters, delimiter escaping and even null values. And there's no real way to fix these issues. I've seen entire processing steps dedicated to re-encoding a CSV file to adjust null values to a downstream systems' preferred null encoding. All of this is especially problematic for empty or small files because there is nothing to infer these things from.</p> <p>What type are the columns if you load an empty CSV file (with headers) into Pandas? This will cause things to break in unexpected ways, e.g. if you're manipulating the DataFrame and call a <code>.dt</code> method it, it will break:</p> <pre><code>from datetime import datetime\nimport pandas as pd\n# fine, even with no rows\npd.DataFrame({'x': [(datetime.now())]})['x'][0:0].dt.hour\n# AttributeError: Can only use .dt accessor with datetimelike values.\npd.DataFrame({'x': [str(datetime.now())]})['x'][0:0].dt.hour\n</code></pre> <p>You'll also encounter issues exporting data, e.g. if you export to a Parquet file that Parquet file will not have the wrong types.</p> <p>And trying to apply types to an existing Pandas DataFrame is tricky. That is partially Pandas' APIs fault, but still the easiest way to avoid these issues it to never end up with a DataFrame of unknown types in the first place.</p>"},{"location":"2023/04/01/data-conversions-in-pandas-arrow-to-the-rescue/#conclusion","title":"Conclusion","text":"<p>I recommend you use Pandas' built in Apache Arrow storage backend or switch to an Arrow native library like Polars. Store your data as Parquet, not CSVs.</p>"},{"location":"2023/04/02/loading-data-into-postgres/","title":"Loading data into Postgres","text":"<p>My last job was at American Family where I was team lead for a property data team. Our data had a wide range of consumers, some of them were data analysts who accessed data via BigQuery while others were ML engineers who trained models off of parquet files in GCS. But most of our users were actually realtime applications serving live humans. As you may know, humans sitting at a keyboard have a pretty short attention span, so low latency was a key requirement for this use case. They also looked up a single address at a time, which would have been expensive and high latency to do via BigQuery or GCS. So we loaded the data into Postgres and served it out from there. So what's the best way to bulk load data into Postgres?</p> <p>But getting data into Postgres from a data warehouse is a non trivial affair. We were cross-cloud so I got to experience the process in both AWS and GCP, here's an overview of what that looks like.</p> <p>In AWS, using Redshift and RDS Postgres, we would use Redshift's built in support to unload a prefix of CSV files. Then we would use RDS' built in S3 extension to load the data from S3 directly into Postgres via a SQL query. Internally this uses Postgres' built in CSV loading, it just manages the S3 auth and streaming.</p> <p>In GCP we used BigQuery and CloudSQL Postgres. Although BigQuery does support exporting CSV files to GCS, GCP's CloudSQL offering doesn't have a built in GCS extension so you're left to spin up some sort of compute and run a small program to stream data from GCS into Postgres.</p> <p>Both of these options use Postgres' built in CSV COPY functionality.</p> <p>While both of these solution work, they're really not great. As soon as you store your data in CSV files you loose all type information and metadata. Not to mention that Postgres may not agree with how the tool you use to export the data does delimiters, quotations, null fields, etc. Of course there's knobs on both export and import to tweak these things, but in my experience that's a huge time sink and sometimes unsolvable. Some more thoughts on CSV as a data interchange format in my last post</p> <p>So what other options do we have for loading data into Postgres?</p>"},{"location":"2023/04/02/loading-data-into-postgres/#loading-via-a-database-driver","title":"Loading via a database driver","text":"<p>One option would be to load the data into Postgres using a database driver in your language of choice. This solves some of the issues because you no longer have to deal with delimiters, quoting things, null values and whatnot. Database drivers also tend to do a good job of converting native language data types (e.g. <code>datetime</code> objects in Python) into Postgres' data format, so type issues are less of a problem.</p> <p>The con of this approach is performance. It's going to be much slower than a bulk load. Possibly even slower if your database driver isn't optimized for these sorts of workflows and encodes the data row by row, making FFI calls to <code>libqp</code> for each item and such.</p> <p>Still, this can be an easy win, especially if you already are running your own compute and database driver like you would have to do for CloudSQL. At the very least you get more control over setting the right types and such on the CSV files, or if the data never had to be in CSV files in the first place and the only reason you made it CSVs was to load it into Postgres then you can sidestep the CSV step altogether.</p>"},{"location":"2023/04/02/loading-data-into-postgres/#loading-another-file-format","title":"Loading another file format","text":"<p>Since the issue is fundamentally that CSVs are under specified and untyped, the obvious solution is to load data via another data format. And the obvious data format is Parquet. So, how can you bulk load a Parquet file into Postgres?</p>"},{"location":"2023/04/02/loading-data-into-postgres/#foreign-data-wrappers","title":"Foreign data wrappers","text":"<p>There's at several option for Parquet FDWs:</p> <ul> <li>parquet_fdw</li> <li>parquet_s3_fdw</li> <li>Probably more</li> </ul> <p>There is unfortunately several downsides to this approach:</p> <ul> <li>Installing Postgres extensions is not simple.</li> <li>Your Postgres database may not let you install extensions. This is pretty much the norm with hosted Postgres solutions, less of an issue with providers like CrunchyData.</li> <li>Giving the FDW running inside of your Postgres instance access to your data can be difficult, e.g. if it's stored on a private bucket. How does the FDW do auth against your bucket provider?</li> <li>This requires you to have written the data out, there's no streaming support.</li> <li>No escape hatch. If the FDW doesn't support the type of column you're working with or you otherwise need to customize things there's no escape hatch, you might be on your own. This is especially dangerous since it might not show up until late into development once the requirements become more complex.</li> </ul>"},{"location":"2023/04/02/loading-data-into-postgres/#copy-with-format-binary","title":"COPY WITH FORMAT BINARY","text":"<p>Did you know that Postgres actually has a standardized binary format you can use to bulk load binary data? It even supports streaming data copies. So what we need is a way to encode data into Postgres' binary format.</p> <p>Since we already wanted to use Parquet as our data storage format and Apache Arrow as our in-memory format the natural thing to do was to encode Arrow data into Postgres' binary format. After some initial prototyping in Python, I ended up writing pgpq, a Rust library that leverages the excellent arrow-rs crate to encode Arrow data into Postgres' binary format.</p> <p>The end result turned out better than I expected:</p> <ul> <li>It's streaming friendly, you can encode anything from a single row to 100k rows at a time.</li> <li>The design is purposefully sans-IO so it works with any database driver, any Postgres host and any data source (it doesn't need to auth against Postgres or the data source).</li> <li>There's excellent interop with Arrow, so you can use Arrow's existing ability to talk directly to object stores or stream larger than memory data.</li> <li>Support for types is vastly superior to that of any FDW that I know of. It can encode nearly all Arrow types, including list arrays and in the future struct arrays.</li> <li>Performance is excellent: I've benchmarked reading, encoding and loading a Parquet file to be faster than loading an equivalent CSV file.</li> <li>Depending on your infra topology offloading CPU use from your database (which is hard to scale and expensive to run compute on) to arbitrary compute can save money and alleviate concerns about overloading your database.</li> </ul> <p>Here's a quick usage example:</p> <pre><code>from tempfile import mkdtemp\nimport psycopg\nimport pyarrow.dataset as ds\nimport requests\nfrom pgpq import ArrowToPostgresBinaryEncoder\n# let's get some example data\ntmpdir = mkdtemp()\nwith open(f\"{tmpdir}/yellow_tripdata_2023-01.parquet\", mode=\"wb\") as f:\nresp = requests.get(\n\"https://d37ci6vzurychx.cloudfront.net/trip-data/yellow_tripdata_2023-01.parquet\"\n)\nresp.raise_for_status()\nf.write(resp.content)\n# load an arrow dataset\n# arrow can load datasets from partitioned parquet files locally or in S3/GCS\n# it handles buffering, matching globs, etc.\ndataset = ds.dataset(tmpdir)\n# create an encoder object which will do the encoding\n# and give us the expected Postgres table schema\nencoder = ArrowToPostgresBinaryEncoder(dataset.schema)\n# get the expected Postgres destination schema\n# note that this is _not_ the same as the incoming arrow schema\n# and not necessarily the schema of your permanent table\n# instead it's the schema of the data that will be sent over the wire\n# which for example does not have timezones on any timestamps\npg_schema = encoder.schema()\n# assemble ddl for a temporary table\n# it's often a good idea to bulk load into a temp table to:\n# (1) Avoid indexes\n# (2) Stay in-memory as long as possible\n# (3) Be more flexible with types\n#     (you can't load a SMALLINT into a BIGINT column without casting)\ncols = [f'\"{col_name}\" {col.data_type.ddl()}' for col_name, col in pg_schema.columns]\nddl = f\"CREATE TEMP TABLE data ({','.join(cols)})\"\nwith psycopg.connect(\"postgres://postgres:postgres@localhost:5432/postgres\") as conn:\nwith conn.cursor() as cursor:\ncursor.execute(ddl)  # type: ignore\nwith cursor.copy(\"COPY data FROM STDIN WITH (FORMAT BINARY)\") as copy:\ncopy.write(encoder.write_header())\nfor batch in dataset.to_batches():\ncopy.write(encoder.write_batch(batch))\ncopy.write(encoder.finish())\n# load into your actual table, possibly doing type casts\n# cursor.execute(\"INSERT INTO \\\"table\\\" SELECT * FROM data\")\n</code></pre>"},{"location":"2023/04/02/loading-data-into-postgres/#type-support","title":"Type support","text":"<p>Here is the current list of supported types:</p> Arrow Postgres Boolean BOOL UInt8 INT2 UInt16 INT4 UInt32 INT8 Int8 CHAR,INT2 Int16 INT2 Int32 INT4 Int64 INT8 Float16 FLOAT4 Float32 FLOAT4 Float64 FLOAT8 Timestamp(Nanosecond) Not supported Timestamp(Microsecond) TIMESTAMP Timestamp(Millisecond) TIMESTAMP Timestamp(Second) TIMESTAMP Date32 DATE Date64 DATE Time32(Millisecond) TIME Time32(Second) TIME Time64(Nanosecond) Not supported Time64(Microsecond) TIME Duration(Nanosecond) Not supported Duration(Microsecond) INTERVAL Duration(Millisecond) INTERVAL Duration(Second) INTERVAL String TEXT,JSONB Binary BYTEA List&lt;T&gt; Array&lt;T&gt; <p>An up to date version of this can be found at the pgpq repo.</p>"},{"location":"2023/04/02/loading-data-into-postgres/#postgres-needs-better-docs","title":"Postgres needs better docs","text":"<p>The hardest part, by far, of writing this library was Postgres' seriously lacking documentation of their binary format. I had to piece it together by reading Postgres COPY docs, the rust-postgres source code and the py-pgproto source code. Postgres' docs and developers tell you to go read the Postgres source code. I just don't see why they can't have a documentation page where they detail the binary format, it really isn't that complicated, it's just poorly documented. Better documentation on the binary protocol would make it a lot easier to write tooling for the Postgres ecosystem.</p>"}]}